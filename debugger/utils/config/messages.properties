[Messages]
nan_loss = LOSS CHECK: Loss is turning into NaN
inf_loss = LOSS CHECK: Loss is turning into +/-Inf
div_loss = LOSS CHECK: Loss is diverging with absolute rate reaching {}
fluctuated_loss = LOSS CHECK: There is a lot of fluctuations, the smoothness of Loss is {} < {}
stagnated_loss = LOSS CHECK: The loss is no-or-slowly decreasing
out_nan = ACTIVATION Check: There was (a) NaN(s) in outputs
out_inf = ACTIVATION Check: There was (a) +/-Inf(s) in outputs
out_cons = ACTIVATION Check: Outputs of the model are constantly unchanged
output_invalid = ACTIVATION Check: The predicted outputs are not valid with respect to the target
act_nan = ACTIVATION Check: There was (a) NaN(s) in activation {}
act_inf = ACTIVATION Check: There was (a) +/-Inf(s) in activation {}
act_ltn = ACTIVATION Check: Activations of {} had values less than {}
act_gtn = ACTIVATION Check: Activations of {} had values greater than {}
act_unstable = ACTIVATION Check: layer activation {} are considered unstable with std of {:.2f} far from [{}, {}]
act_dead = ACTIVATION Check: {}/{} neuron(s) (or filter(s)) of FC (or conv) layer activation {} are considered dead
act_sat = ACTIVATION Check: {}/{} neuron(s) (filter(s)) of FC  (or conv) layer activation {} are considered saturated
b_nan = BIAS CHECK: There was (a) NaN(s) in bias {}
b_inf = BIAS CHECK: There was (a) +/-Inf(s) in bias : {}
b_div_1 = BIAS CHECK: Bias {} is diverging until reaching abs mean of {} > {}
b_div_2 = BIAS CHECK: Bias {} is diverging with growth rate reaching {} > {}
w_nan = WEIGHT CHECK: There was (a) NaN(s) in weight {}
w_inf = WEIGHT CHECK: There was (a) +/-Inf(s) in weight {}
conv_w_sign =  WEIGHT CHECK: Conv. layer weight {} has {} as ratio of negative elements (>{})
conv_w_dead = WEIGHT CHECK: Conv. Layer weight {} has {} as ratio of dead elements (<={})
conv_w_div_1 = WEIGHT CHECK: Conv. layer weight are diverging until reaching abs mean of {} > {}
conv_w_div_2 = WEIGHT CHECK: Conv. layer weight are diverging with growth rate reaching {} > {}
need_bias = BIAS CHECK: The model missed biases. Do not consider this alert if you use batchnorm for all layers
last_bias = BIAS CHECK: Bias of last layer should not be zero, in case of unbalanced data
ineff_bias_cls = BIAS CHECK: Bias of last layer should match the ratio of targets
zero_bias = BIAS CHECK: It is recommended to choose null biases (zeros)
grad_err = GRADIENT CHECK: Gradient seems to be incorrect with numerical error >  than {}
poor_init_loss = LOSS CHECK: Loss at cold start is considered poor and problematic: relative error of {}
poor_reduction_loss = LOSS CHECK: The reduction of the loss is poorly designed: it is recommended to use AVG instead
observations_constant = STATES CHECK: States are constant
observations_unnormalized = STATES CHECK: States seem to be unnormalized
Data_dep = PROPER FITTING CHECK: The training procedure seems to be not considering the data inputs
underfitting_single_batch = PROPER FITTING CHECK: The DNN training is unable to fit properly a single batch of data
zero_loss = PROPER FITTING CHECK: The loss is smoothly decreasing towards zero: The model may need regularization
poor_init = WEIGHT CHECK: Poor initialization (unbreaking symmetry) of weight {}
need_he = WEIGHT CHECK: It is recommended to choose He initialization for weight {} with RELU: error of {}
need_glorot = WEIGHT CHECK: It is recommended to choose Glorot/Xavier initialization for weight {} with Tanh: error of {}
need_lecun = WEIGHT CHECK: It is recommended to choose Lecun initialization for weight {} with Sigmoid: error of {}
need_init_well = WEIGHT CHECK: It is recommended to choose a well-known initialization for weight {}
entropy_start = ACTION CHECK: The average entropy of action probabilities start low with value : {:.2f}
entropy_incr = ACTION CHECK: the entropy is increasing over time with an increase rate of {:.2f}.
entropy_stag = ACTION CHECK: the entropy is stagnated over time with an evolution slope equal to {:.2f} . The agent's learning process is not stable.
entropy_strong_dec = ACTION CHECK: The second derivative of entropy is less than {}, indicating that entropy is decreasing very fast.
entropy_fluctuation = ACTION CHECK: the entropy is fluctuating over time with a mse {:.2f} > {}.
mcd_uncertainty = ACTION CHECK: the Mont Carlo Dropout uncertainty is high; with average action std {:.2f} > {} over {} repetitions. Your agent is not learning correctly.
invalid_step_func = ENVIRONMENT CHECK: The step function is returning similar states with an average std {}.
weak_reward_threshold = ENVIRONMENT CHECK: The reward threshold for the environment may be too low. The maximum reward has been reached with random actions.
decreasing_reward = REWARD CHECK: After {} %% of your total steps the reward is still not stagnating.
stagnated_reward = REWARD CHECK: After {} %% of your total steps the reward has stagnated at {:.2f} while the maximum reward is {}.
fluctuated_reward = REWARD CHECK: In the first {} %% of the total training steps the reward has a low variability.
target_network_not_updated = AGENT CHECK: The target network's parameters have not been updated when reaching the update period.
similar_target_and_main_network = AGENT CHECK: The parameters of the main and target networks are similar.
using_the_wrong_network= AGENT CHECK: You are using the wrong model to do the predictions (Hint: verify that you are not using the target model to choose the action).
q_target_err = Q Target CHECK: the calculation of Q-learning target values seems to be incorrect.
bounded_observations = ENVIRONMENT CHECK: Observation space must be a Box or Discrete.
bounded_actions = ENVIRONMENT CHECK: Action space must be a Box or Discrete.
wrong_reset_func = ENVIRONMENT CHECK: Reset function must return an initial state.
observation_not_returned= ENVIRONMENT CHECK: Step function must return an observation.
non_bool_done= ENVIRONMENT CHECK: Step function must return a boolean done flag.
reward_not_numerical= ENVIRONMENT CHECK: The reward must be a numerical value.
max_episode_steps_not_numerical= ENVIRONMENT CHECK: The max steps per episode must be a numerical value.
reward_threshold_not_numerical= ENVIRONMENT CHECK: The reward threshold per episode must be a numerical value.
observations_are_similar = STATES CHECK: The States in the last {} steps of the last episode are similar.
bad_exploration_param_initialization = EXPLORATION PARAMETER CHECK: The exploration factor has been initialized to {} while it should start from {}
increasing_exploration_factor = EXPLORATION PARAMETER CHECK: The exploration factor value is increasing.
decreasing_exploration_factor = EXPLORATION PARAMETER CHECK: The exploration factor value is decreasing.
quick_changing_exploration_factor = EXPLORATION PARAMETER CHECK: The exploration factor value is changing very quickly, try reducing its decay.
stagnating_exploration_factor = EXPLORATION PARAMETER CHECK: The exploration factor value is stagnated.
steps_are_not_changing = STEPS CHECK: The steps number is not incrementing
observations_are_stagnating =  STATES CHECK: The states are stagnating in the last steps of the episodes.
poor_max_step_per_ep = STEPS CHECK: Poor value of the max step per episode
actions_are_similar = ACTION CHECK: The actions in the last {}  episode are similar.
kl_div_high = AGENT CHECK:  the KL divergence's value is high: {:.2f} > {}.
reward_unnormalized = ENVIRONMENT CHECK: the reward seems to be unnormalized
action_stagnation = ACTION CHECK: In the last episodes, the agent chose a single action more than {:.2f} percent of the time.
target_network_changing = AGENT CHECK: the target network params are changing in the wrong period