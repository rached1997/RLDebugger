[Messages]
nan_loss = LOSS CHECK: Loss is turning into NaN
inf_loss = LOSS CHECK: Loss is turning into +/-Inf
div_loss = LOSS CHECK: Loss is diverging with absolute rate reaching {}
fluctuated_loss = LOSS CHECK: There is a lot of fluctuations, the smoothness of Loss is {} < {}
stagnated_loss = LOSS CHECK: The loss is no-or-slowly decreasing
out_nan = ACTIVATION Check: There was (a) NaN(s) in outputs
out_inf = ACTIVATION Check: There was (a) +/-Inf(s) in outputs
out_cons = ACTIVATION Check: Outputs of the model are constantly unchanged
output_invalid = ACTIVATION Check: The predicted outputs are not valid with respect to the target
act_nan = ACTIVATION Check: There was (a) NaN(s) in activation {}
act_inf = ACTIVATION Check: There was (a) +/-Inf(s) in activation {}
act_ltn = ACTIVATION Check: Activations of {} had values less than {}
act_gtn = ACTIVATION Check: Activations of {} had values greater than {}
act_unstable = ACTIVATION Check: layer activation {} are considered unstable with std of {} far from [{}, {}]
act_dead = ACTIVATION Check: {}/{} neuron(s) (or filter(s)) of FC (or conv) layer activation {} are considered dead
act_sat = ACTIVATION Check: {}/{} neuron(s) (filter(s)) of FC  (or conv) layer activation {} are considered saturated
b_nan = BIAS CHECK: There was (a) NaN(s) in bias {}
b_inf = BIAS CHECK: There was (a) +/-Inf(s) in bias : {}
b_div_1 = BIAS CHECK: Bias {} is diverging until reaching abs mean of {} > {}
b_div_2 = BIAS CHECK: Bias {} is diverging with growth rate reaching {} > {}
w_nan = WEIGHT CHECK: There was (a) NaN(s) in weight {}
w_inf = WEIGHT CHECK: There was (a) +/-Inf(s) in weight {}
conv_w_sign =  WEIGHT CHECK: Conv. layer weight {} has {} as ratio of negative elements (>{})
conv_w_dead = WEIGHT CHECK: Conv. Layer weight {} has {} as ratio of dead elements (<={})
conv_w_div_1 = WEIGHT CHECK: Conv. layer weight are diverging until reaching abs mean of {} > {}
conv_w_div_2 = WEIGHT CHECK: Conv. layer weight are diverging with growth rate reaching {} > {}
need_bias = BIAS CHECK: The model missed biases. Do not consider this alert if you use batchnorm for all layers
last_bias = BIAS CHECK: Bias of last layer should not be zero, in case of unbalanced data
ineff_bias_cls = BIAS CHECK: Bias of last layer should match the ratio of labels
zero_bias = BIAS CHECK: It is recommended to choose null biases (zeros)
grad_err = GRADIENT CHECK: Gradient seems to be incorrect with numerical error >  than {}
poor_init_loss = LOSS CHECK: Loss at cold start is considered poor and problematic: relative error of {}
poor_reduction_loss = LOSS CHECK: The reduction of the loss is poorly designed: it is recommended to use AVG instead
observations_constant = OBSERVATION CHECK: Observations are constant
observations_unnormalized = OBSERVATION CHECK: Observations seem to be unnormalized
Data_dep = PROPER FITTING CHECK: The training procedure seems to be not considering the data inputs
underfitting_single_batch = PROPER FITTING CHECK: The DNN training is unable to fit properly a single batch of data
zero_loss = PROPER FITTING CHECK: The loss is smoothly decreasing towards zero: The model may need regularization
poor_init = WEIGHT CHECK: Poor initialization (unbreaking symmetry) of weight {}
need_he = WEIGHT CHECK: It is recommended to choose He initialization for weight {} with RELU: error of {}
need_glorot = WEIGHT CHECK: It is recommended to choose Glorot/Xavier initialization for weight {} with Tanh: error of {}
need_lecun = WEIGHT CHECK: It is recommended to choose Lecun initialization for weight {} with Sigmoid: error of {}
need_init_well = WEIGHT CHECK: It is recommended to choose a well-known initialization for weight {}